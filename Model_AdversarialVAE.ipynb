{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmbost20/YouTransfer/blob/main/453_Group5_Cloned_Git_Approach.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYJE9MMyrHPH",
        "outputId": "5f3b5d39-947d-40a7-b318-aebf61514829"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        }
      ],
      "source": [
        "#@title Setup\n",
        "!pip install transformers\n",
        "\n",
        "#numpy==1.17.2\n",
        "#torch==1.3.0\n",
        "#transformers==2.1.1\n",
        "##sklearn==0.0\n",
        "#spacy==2.2.3\n",
        "#nltk==3.4.5\n",
        "#gensim==3.8.1\n",
        "#tqdm==4.36.1\n",
        "\n",
        "#Imports\n",
        "import transformers\n",
        "import sklearn\n",
        "import spacy\n",
        "import nltk\n",
        "import gensim\n",
        "import tqdm\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "#from linguistic_style_transfer_pytorch.config import GeneralConfig, ModelConfig\n",
        "#from linguistic_style_transfer_pytorch.data_loader import TextDataset\n",
        "#from linguistic_style_transfer_pytorch.model import AdversarialVAE\n",
        "from tqdm import tqdm, trange\n",
        "import os\n",
        "import numpy as np\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uawtDjszlY7",
        "outputId": "f2f97a7c-4ed7-46aa-e64c-65b0930a72ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KU5BLJtRnkV2",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Git Clone\n",
        "#!git clone https://github.com/h3lio5/linguistic-style-transfer-pytorch.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUjy9kcDtfOH"
      },
      "outputs": [],
      "source": [
        "#@title Configs\n",
        "\n",
        "#### THESE ARE CONFIG FUNCTION FROM THIS GITHUB (https://github.com/h3lio5/linguistic-style-transfer-pytorch)\n",
        "\n",
        "import os\n",
        "\n",
        "ROOT = '/content/linguistic-style-transfer-pytorch' #os.path.join(os.getcwd(),'linguistic_style_transfer_pytorch')\n",
        "\n",
        "\n",
        "class GeneralConfig:\n",
        "    \"\"\"\n",
        "    General configuration\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.vocab_size = 10000\n",
        "        self.bow_hidden_dim = 10000\n",
        "        #self.data_path = os.path.join(\n",
        "        #    ROOT, \"linguistic_style_transfer_pytorch\", \"data\")\n",
        "        #self.vocab_save_path = os.path.join(\n",
        "        #    ROOT, \"linguistic_style_transfer_pytorch\", \"data\")\n",
        "        #self.train_pos_reviews_file_path = os.path.join(\n",
        "        #    ROOT, \"linguistic_style_transfer_pytorch\", \"data\", \"raw\", \"sentiment.train.1.txt\")\n",
        "        #self.train_neg_reviews_file_path = os.path.join(\n",
        "        #    ROOT, \"linguistic_style_transfer_pytorch\", \"data\", \"raw\", \"sentiment.train.0.txt\")\n",
        "        #self.train_text_file_path = os.path.join(\n",
        "            #ROOT, \"linguistic_style_transfer_pytorch\", \"data\", \"clean\", \"Corpus.npy\") # **key** Changed\n",
        "        self.train_text_file_path = '/content/drive/MyDrive/Style-Transfer/Data/Corpus.npy'\n",
        "        #self.train_labels_file_path = os.path.join(\n",
        "         #   ROOT, \"linguistic_style_transfer_pytorch\", \"data\", \"clean\", \"labels.npy\") # **key** Changed\n",
        "        self.train_labels_file_path = '/content/drive/MyDrive/Style-Transfer/Data/labels.npy' \n",
        "        #self.pos_sentiment_file_path = os.path.join(\n",
        "        #    ROOT, \"linguistic_style_transfer_pytorch\", \"data\", \"lexicon\", \"positive-words.txt\")\n",
        "        #self.neg_sentiment_file_path = os.path.join(\n",
        "        #    ROOT, \"linguistic_style_transfer_pytorch\", \"data\", \"lexicon\", \"negative-words.txt\")\n",
        "        #self.word_embedding_text_file_path = os.path.join(\n",
        "        #    ROOT, \"linguistic_style_transfer_pytorch\", \"data\", \"embedding.txt\")\n",
        "        #self.word_embedding_path = os.path.join(\n",
        "         #   ROOT, \"linguistic_style_transfer_pytorch\", \"data\", \"word_embeddings.npy\") # **key** Changed\n",
        "        self.word_embedding_path = '/content/drive/MyDrive/Style-Transfer/Data/embedding_matrix.npy'\n",
        "        #self.vocab_file_path = os.path.join(\n",
        "        #    ROOT, \"linguistic_style_transfer_pytorch\", \"data\", \"Vocabulary_obj.pth\") # **key** Changed\n",
        "        self.vocab_file_path = '/content/drive/MyDrive/Style-Transfer/Data/Vocabulary_obj.pth'\n",
        "        #self.bow_file_path = os.path.join(\n",
        "         #   ROOT, \"linguistic_style_transfer_pytorch\", \"data\", \"BoW_file.json\") # **key** Changed\n",
        "        self.bow_file_path = '/content/drive/MyDrive/Style-Transfer/Data/BoW_file.json'\n",
        "        #self.model_save_path = os.path.join(\n",
        "         #   ROOT, \"linguistic_style_transfer_pytorch\", \"checkpoints\")\n",
        "        self.model_save_path = '/content/drive/MyDrive/Style-Transfer/Models/checkpoints'\n",
        "        self.avg_style_emb_path = '/content/drive/MyDrive/Style-Transfer/Models/checkpoints/avg_style_emb.pkl'\n",
        "        self.embedding_size = 64\n",
        "        self.pad_token = 0\n",
        "        self.sos_token = 1\n",
        "        self.unk_token = 2\n",
        "        self.predefined_word_index = {\n",
        "            \"<pad>\": 0,\n",
        "            \"<sos>\": 1,\n",
        "            \"<unk>\": 2,\n",
        "        }\n",
        "        self.filter_sentiment_words = True\n",
        "        self.filter_stopwords = True\n",
        "\n",
        "\n",
        "class ModelConfig:\n",
        "    \"\"\"\n",
        "    Model configuration\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # vocab size after including special tokens\n",
        "        self.vocab_size = 10000\n",
        "        self.epochs = 20\n",
        "        # batch setting\n",
        "        self.batch_size = 128\n",
        "        # layer sizes\n",
        "        self.embedding_size = 64\n",
        "        self.hidden_dim = 256\n",
        "        self.style_hidden_dim = 8 # **key** Going to want to increase as we add more youtubers but keep smaller than content\n",
        "        self.content_hidden_dim = 128\n",
        "        # generative embedding dim = style_hidden_dim + content_hidden_dim\n",
        "        self.generative_emb_dim = 136\n",
        "        self.num_style = 2 # **key** Of youtubers we currently have. Fine at 2 for now.\n",
        "        self.content_bow_dim = 10000\n",
        "        # dropout\n",
        "        self.dropout = 0.8\n",
        "        # sequence length settings\n",
        "        self.max_seq_len = 20 # originally 15, changed to 20 from 75\n",
        "        # learning rates\n",
        "        self.autoencoder_lr = 0.001\n",
        "        self.style_adversary_lr = 0.001\n",
        "        self.content_adversary_lr = 0.001\n",
        "        # loss weights\n",
        "        self.style_multitask_loss_weight = 10\n",
        "        self.content_multitask_loss_weight = 3\n",
        "        self.style_adversary_loss_weight = 1\n",
        "        self.content_adversary_loss_weight = 0.03\n",
        "        self.style_kl_lambda = 0.03\n",
        "        self.content_kl_lambda = 0.03\n",
        "        # kl annealing max iterations\n",
        "        self.kl_anneal_iterations = 20000\n",
        "        # noise\n",
        "        self.epsilon = 1e-8\n",
        "        self.label_smoothing = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxQbd7Hxtis3"
      },
      "outputs": [],
      "source": [
        "#@title Text Datasets\n",
        "\n",
        "#### THIS IS TEXT DATASET FUNCTIONS FROM THIS GITHUB (https://github.com/h3lio5/linguistic-style-transfer-pytorch)\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "import json\n",
        "\n",
        "gconfig = GeneralConfig()\n",
        "mconfig = ModelConfig()\n",
        "\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for text data\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, mode='train'):\n",
        "        super(TextDataset, self).__init__()\n",
        "        # load train data\n",
        "        #with open(gconfig.train_text_file_path) as f:\n",
        "        #    self.train_data = f.readlines()\n",
        "        self.train_data = np.load(gconfig.train_text_file_path, allow_pickle = True)\n",
        "        print(self.train_data.shape)\n",
        "        # load train labels\n",
        "        #with open(gconfig.train_labels_file_path) as f:\n",
        "        #    self.train_labels = f.readlines()\n",
        "        self.train_labels = np.load(gconfig.train_labels_file_path)\n",
        "        # load vocab\n",
        "        #with open(gconfig.vocab_file_path) as f:\n",
        "        #    self.vocab = torch.load(f)\n",
        "        self.vocab = torch.load(gconfig.vocab_file_path)\n",
        "        # load bow vocab\n",
        "        #with open(gconfig.bow_file_path) as f:\n",
        "        #   self.bow_filtered_vocab_indices = json.load(f)\n",
        "        self.bow_filtered_vocab_indices = self.vocab\n",
        "        self.label2index = {'Kings and Generals': [0, 1], '3Blue1Brown': [1, 0]} # **key**  One hot encoded\n",
        "\n",
        "    def _padding(self, token_ids):\n",
        "        \"\"\"\n",
        "        Utility function to add padding to and trim the sentence\n",
        "        \"\"\"\n",
        "        if len(token_ids) > mconfig.max_seq_len:\n",
        "            return token_ids[:mconfig.max_seq_len]\n",
        "        token_ids = token_ids + \\\n",
        "            (mconfig.max_seq_len-len(token_ids))*[gconfig.pad_token]\n",
        "\n",
        "        return token_ids\n",
        "\n",
        "    def _sentence_tokenid(self, sentence):\n",
        "        \"\"\"\n",
        "        Returns token ids of individual words of the sentence\n",
        "        \"\"\"\n",
        "        token_ids = [self.vocab[word] if self.vocab[word] != -1 else self.vocab['<unk>'] for word in sentence]\n",
        "        padded_token_ids = self._padding(token_ids)\n",
        "        return padded_token_ids, len(token_ids)\n",
        "\n",
        "    def _get_bow_representations(self, text_sequence):\n",
        "        \"\"\"\n",
        "        Returns BOW representation of every sequence of the batch\n",
        "        \"\"\"\n",
        "\n",
        "        sequence_bow_representation = np.zeros(\n",
        "            shape=gconfig.bow_hidden_dim, dtype=np.float32)\n",
        "        # Iterate over each word in the sequence\n",
        "        for index in text_sequence:\n",
        "            if index in self.bow_filtered_vocab_indices:\n",
        "                bow_index = self.bow_filtered_vocab_indices[index]\n",
        "                sequence_bow_representation[bow_index] += 1\n",
        "        sequence_bow_representation /= np.max(\n",
        "            [np.sum(sequence_bow_representation), 1])\n",
        "\n",
        "        return np.asarray(sequence_bow_representation)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the total number of samples\n",
        "        \"\"\"\n",
        "        return len(self.train_labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            token_ids: token ids of the sentence\n",
        "            seq_len : length of the sentence before padding\n",
        "            label   : label of the sentence\n",
        "            bow_rep : Bag of Words representation of the sentence\n",
        "        \"\"\"\n",
        "        sentence = self.train_data[index]\n",
        "        label = self.label2index[self.train_labels[index].strip()]\n",
        "        token_ids, seq_len = self._sentence_tokenid(sentence)\n",
        "        bow_rep = self._get_bow_representations(sentence)\n",
        "        return (torch.LongTensor(token_ids), torch.LongTensor([seq_len]), torch.LongTensor(label), torch.FloatTensor(bow_rep))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cqja84_Ao5_b",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Preprocess code\n",
        "import re\n",
        "import logging\n",
        "#from linguistic_style_transfer_pytorch.config import GeneralConfig\n",
        "\n",
        "config = GeneralConfig()\n",
        "\n",
        "\n",
        "class Preprocessor():\n",
        "    \"\"\"\n",
        "    Preprocessor class\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        print(\"Preprocessor instantiated\")\n",
        "\n",
        "    def _clean_text(self, string):\n",
        "        \"\"\"\n",
        "        Clean the raw text file\n",
        "        \"\"\"\n",
        "        string = string.replace(\".\", \"\")\n",
        "        string = string.replace(\".\", \"\")\n",
        "        string = string.replace(\"\\n\", \" \")\n",
        "        string = string.replace(\" 's\", \" is\")\n",
        "        string = string.replace(\"'m\", \" am\")\n",
        "        string = string.replace(\"'ve\", \" have\")\n",
        "        string = string.replace(\"n't\", \" not\")\n",
        "        string = string.replace(\"'re\", \" are\")\n",
        "        string = string.replace(\"'d\", \" would\")\n",
        "        string = string.replace(\"'ll\", \" will\")\n",
        "        string = string.replace(\"\\r\", \" \")\n",
        "        string = string.replace(\"\\n\", \" \")\n",
        "        string = re.sub(r'\\d+', \"number\", string)\n",
        "        string = ''.join(x for x in string if x.isalnum() or x == \" \")\n",
        "        string = re.sub(r'\\s{2,}', \" \", string)\n",
        "        string = string.strip().lower()\n",
        "\n",
        "        return string\n",
        "\n",
        "    def preprocess(self):\n",
        "        \"\"\"\n",
        "        Preprocesses the train text data \n",
        "        \"\"\"\n",
        "        print(\"Preprocessing started\")\n",
        "        with open(config.train_text_file_path, 'w') as text_file, open(config.train_labels_file_path, 'w') as labels_file:\n",
        "            with open(config.train_pos_reviews_file_path, 'r') as reviews_file:\n",
        "                for line in reviews_file:\n",
        "                    line = self._clean_text(line)\n",
        "                    if len(line) > 0:\n",
        "                        text_file.write(line + \"\\n\")\n",
        "                        labels_file.write(\"pos\" + \"\\n\")\n",
        "            with open(config.train_neg_reviews_file_path, 'r') as reviews_file:\n",
        "                for line in reviews_file:\n",
        "                    line = self._clean_text(line)\n",
        "                    if len(line) > 0:\n",
        "                        text_file.write(line + \"\\n\")\n",
        "                        labels_file.write(\"pos\" + \"\\n\")\n",
        "        print(\"Processing complete \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23LNCQgEtoFt"
      },
      "outputs": [],
      "source": [
        "#@title AdversarialVAE\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "mconfig = ModelConfig()\n",
        "gconfig = GeneralConfig()\n",
        "\n",
        "\n",
        "class AdversarialVAE(nn.Module):\n",
        "    \"\"\"\n",
        "    Model architecture defined according to the paper\n",
        "    'Disentangled Representation Learning for Non-Parallel Text Style Transfer'\n",
        "    https://www.aclweb.org/anthology/P19-1041.pdf\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, weight):\n",
        "        \"\"\"\n",
        "        Initialize networks\n",
        "        \"\"\"\n",
        "        super(AdversarialVAE, self).__init__()\n",
        "        # word embeddings\n",
        "        self.embedding = nn.Embedding.from_pretrained(weight)\n",
        "        #================ Encoder model =============#\n",
        "        self.encoder = nn.GRU(\n",
        "            mconfig.embedding_size, mconfig.hidden_dim, batch_first=True, bidirectional=True)\n",
        "        # content latent embedding\n",
        "        self.content_mu = nn.Linear(\n",
        "            2*mconfig.hidden_dim, mconfig.content_hidden_dim)  \n",
        "        self.content_log_var = nn.Linear(\n",
        "            2*mconfig.hidden_dim, mconfig.content_hidden_dim)\n",
        "        # style latent embedding\n",
        "        self.style_mu = nn.Linear(\n",
        "            2*mconfig.hidden_dim, mconfig.style_hidden_dim)\n",
        "        self.style_log_var = nn.Linear(\n",
        "            2*mconfig.hidden_dim, mconfig.style_hidden_dim)\n",
        "        #=============== Discriminator/adversary============#\n",
        "        self.style_disc = nn.Linear(\n",
        "            mconfig.content_hidden_dim, mconfig.num_style)\n",
        "        self.content_disc = nn.Linear(\n",
        "            mconfig.style_hidden_dim, mconfig.content_bow_dim)\n",
        "        #=============== Classifier =============#\n",
        "        self.content_classifier = nn.Linear(\n",
        "            mconfig.content_hidden_dim, mconfig.content_bow_dim)\n",
        "        self.style_classifier = nn.Linear(\n",
        "            mconfig.style_hidden_dim, mconfig.num_style)\n",
        "        #=============== Decoder =================#\n",
        "        # Note: input embeddings are concatenated with the sampled latent vector at every step\n",
        "        self.decoder = nn.GRUCell(\n",
        "            mconfig.embedding_size + mconfig.generative_emb_dim, mconfig.hidden_dim)\n",
        "        self.projector = nn.Linear(mconfig.hidden_dim, mconfig.vocab_size)\n",
        "        #============== Average label embedding ======#\n",
        "        # Used during inference to transfer style.\n",
        "        # Each element of the dict consists of average of latent style embeddings\n",
        "        # of all the sentences of that particular label/style.\n",
        "\n",
        "        # **key** t0 -> negative, 1 -> positive\n",
        "        # USE THIS TO CLASSIFY THE AUTHORS OF THE YOUTUBE VIDEOS\n",
        "        self.avg_style_emb = {\n",
        "            0: torch.zeros(mconfig.style_hidden_dim),\n",
        "            1: torch.zeros(mconfig.style_hidden_dim)\n",
        "        }\n",
        "        # Used to maintain a running average\n",
        "        self.num_neg_styles = 0\n",
        "        self.num_pos_styles = 0\n",
        "        # dropout\n",
        "        self.dropout = nn.Dropout(mconfig.dropout)\n",
        "\n",
        "    def forward(self, sequences, seq_lengths, style_labels, content_bow, iteration, last_epoch):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            sequences : token indices of input sentences of shape = (batch_size,max_seq_length)\n",
        "            seq_lengths: actual lengths of input sentences before padding, shape = (batch_size,1)\n",
        "            style_labels: labels of sentiment of the input sentences, shape = (batch_size,2)\n",
        "            content_bow: Bag of Words representations of the input sentences, shape = (batch_size,bow_hidden_size)\n",
        "            iteration: number of iterations completed till now; used for KL annealing\n",
        "            last_epoch: save average style embeddings if last_epoch is true\n",
        "        Returns:\n",
        "            content_disc_loss: loss incurred by content discriminator/adversary\n",
        "            style_disc_loss  : loss incurred by style discriminator/adversary\n",
        "            vae_and_classifier_loss : consists of loss incurred by autoencoder, content and style\n",
        "                                      classifiers\n",
        "        \"\"\"\n",
        "        # pack the sequences to reduce unnecessary computations\n",
        "        # It requires the sentences to be sorted in descending order to take\n",
        "        # full advantage\n",
        "        seq_lengths, perm_index = seq_lengths.sort(descending=True)\n",
        "        sequences = sequences[perm_index]\n",
        "        embedded_seqs = self.dropout(self.embedding(sequences))\n",
        "        print(\"pre encoder embedded_seqs size: \", embedded_seqs.size())\n",
        "        print(\"seq_lengths: \", seq_lengths.size())\n",
        "        packed_seqs = pack_padded_sequence(\n",
        "            embedded_seqs, lengths=seq_lengths, batch_first=True)\n",
        "        print(\"packed seqs: \", packed_seqs.data.size())\n",
        "        packed_output, (_) = self.encoder(packed_seqs)\n",
        "        print(\"packed_output: \", packed_output.data.size())\n",
        "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
        "        print(\"output: \", output.size())\n",
        "        sentence_emb = output[torch.arange(output.size(0)), seq_lengths-1]\n",
        "        # get content and style embeddings from the sentence embeddings,i.e. final_hidden_state\n",
        "        print(\"after encoder sentence_emb: \", sentence_emb.size())\n",
        "        content_emb_mu, content_emb_log_var = self.get_content_emb(\n",
        "            sentence_emb)\n",
        "        style_emb_mu, style_emb_log_var = self.get_style_emb(\n",
        "            sentence_emb)\n",
        "        # sample content and style embeddings from their respective latent spaces\n",
        "        sampled_content_emb = self.sample_prior(\n",
        "            content_emb_mu, content_emb_log_var)\n",
        "        sampled_style_emb = self.sample_prior(\n",
        "            style_emb_mu, style_emb_log_var)\n",
        "        # Generative embedding\n",
        "        generative_emb = torch.cat(\n",
        "            (sampled_style_emb, sampled_content_emb), axis=1)\n",
        "        # Update the average style embeddings for different styles\n",
        "        # This will be used in transfering the style of a sentence\n",
        "        # during inference\n",
        "        \n",
        "        #if last_epoch:\n",
        "        #print(\"pre\")\n",
        "        self.update_average_style_emb(sampled_style_emb, style_labels)\n",
        "        #print(\"post\")\n",
        "        #print(\"now: \", self.avg_style_emb)\n",
        "\n",
        "        #=========== Losses on content space =============#\n",
        "        # Discriminator Loss\n",
        "        content_disc_preds = self.get_content_disc_preds(sampled_style_emb)\n",
        "        content_disc_loss = self.get_content_disc_loss(\n",
        "            content_disc_preds, content_bow)\n",
        "        # adversarial entropy\n",
        "        content_entropy_loss = self.get_entropy_loss(content_disc_preds)\n",
        "        # Multitask loss\n",
        "        content_mul_loss = self.get_content_mul_loss(\n",
        "            sampled_content_emb, content_bow)\n",
        "\n",
        "        #============ Losses on style space ================#\n",
        "        # Discriminator loss\n",
        "        style_disc_preds = self.get_style_disc_preds(sampled_content_emb)\n",
        "        style_disc_loss = self.get_style_disc_loss(\n",
        "            style_disc_preds, style_labels)\n",
        "        # adversarial entropy\n",
        "        style_entropy_loss = self.get_entropy_loss(style_disc_preds)\n",
        "        # Multitask loss\n",
        "        style_mul_loss = self.get_style_mul_loss(\n",
        "            sampled_style_emb, style_labels)\n",
        "\n",
        "        #============== KL losses ===========#\n",
        "        # Style space\n",
        "        style_kl_loss = self.get_kl_loss(\n",
        "            style_emb_mu, style_emb_log_var)\n",
        "        if iteration < mconfig.kl_anneal_iterations:\n",
        "            style_kl_loss = self.get_annealed_weight(\n",
        "                iteration, mconfig.style_kl_lambda) * style_kl_loss\n",
        "        # Content space\n",
        "        content_kl_loss = self.get_kl_loss(\n",
        "            content_emb_mu, content_emb_log_var)\n",
        "        if iteration < mconfig.kl_anneal_iterations:\n",
        "            content_kl_loss = self.get_annealed_weight(\n",
        "                iteration, mconfig.content_kl_lambda) * content_kl_loss\n",
        "\n",
        "        #=============== reconstruction ================#\n",
        "        reconstructed_sentences = self.generate_sentences(\n",
        "            sequences, generative_emb)\n",
        "        reconstruction_loss = self.get_recon_loss(\n",
        "            reconstructed_sentences, sequences)\n",
        "        #================ total weighted loss ==========#\n",
        "        vae_and_classifier_loss = mconfig.content_adversary_loss_weight * content_entropy_loss + \\\n",
        "            mconfig.style_adversary_loss_weight * style_entropy_loss + \\\n",
        "            mconfig.style_multitask_loss_weight * style_mul_loss + \\\n",
        "            mconfig.content_multitask_loss_weight * content_mul_loss + \\\n",
        "            reconstruction_loss + style_kl_loss + content_kl_loss\n",
        "\n",
        "        return content_disc_loss, style_disc_loss, vae_and_classifier_loss\n",
        "\n",
        "    def get_style_content_emb(self, sequences, seq_lengths, style_labels, content_bow):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            sequences : token indices of input sentences of shape = (batch_size,max_seq_length)\n",
        "            seq_lengths: actual lengths of input sentences before padding, shape = (batch_size,1)\n",
        "            style_labels: labels of sentiment of the input sentences, shape = (batch_size,2)\n",
        "            content_bow: Bag of Words representations of the input sentences, shape = (batch_size,bow_hidden_size)\n",
        "        Returns:\n",
        "            sampled_content_emb: content embeddings sampled from the content latent space, shape=(batch_size,content_hid_dim)\n",
        "            sampled_style_emb:   style embeddings sampled from the style latent space, shape=(batch_size,style_hid_dim)\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            # pack the sequences to reduce unnecessary computations\n",
        "            # It requires the sentences to be sorted in descending order to take\n",
        "            # full advantage\n",
        "            seq_lengths, perm_index = seq_lengths.sort(descending=True)\n",
        "            sequences = sequences[perm_index]\n",
        "            embedded_seqs = self.embedding(sequences)\n",
        "            packed_seqs = pack_padded_sequence(\n",
        "                embedded_seqs, lengths=seq_lengths, batch_first=True)\n",
        "            packed_output, (_) = self.encoder(packed_seqs)\n",
        "            output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
        "            sentence_emb = output[torch.arange(output.size(0)), seq_lengths-1]\n",
        "            # get content and style embeddings from the sentence embeddings,i.e. final_hidden_state\n",
        "            content_emb_mu, content_emb_log_var = self.get_content_emb(\n",
        "                sentence_emb)\n",
        "            style_emb_mu, style_emb_log_var = self.get_style_emb(\n",
        "                sentence_emb)\n",
        "            # sample content and style embeddings from their respective latent spaces\n",
        "            sampled_content_emb = self.sample_prior(\n",
        "                content_emb_mu, content_emb_log_var)\n",
        "            sampled_style_emb = self.sample_prior(\n",
        "                style_emb_mu, style_emb_log_var)\n",
        "\n",
        "        return sampled_content_emb, sampled_style_emb\n",
        "\n",
        "    def get_params(self):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            content_disc_params: parameters of the content discriminator/adversary\n",
        "            style_disc_params  : parameters of the style discriminator/adversary\n",
        "            other_params       : parameters of the vae and classifiers\n",
        "        \"\"\"\n",
        "\n",
        "        content_disc_params = self.content_disc.parameters()\n",
        "        style_disc_params = self.style_disc.parameters()\n",
        "        other_params = list(self.encoder.parameters()) + list(self.decoder.parameters()) + \\\n",
        "            list(self.style_classifier.parameters()) + \\\n",
        "            list(self.content_classifier.parameters())\n",
        "\n",
        "        return content_disc_params, style_disc_params, other_params\n",
        "\n",
        "    def get_content_emb(self, sentence_emb):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            sentence_emb: sentence embeddings of all the sentences in the batch, shape=(batch_size,2*gru_hidden_dim)\n",
        "        Returns:\n",
        "            mu: embedding of the mean of the Gaussian distribution of the content's latent space\n",
        "            log_var: embedding of the log of variance of the Gaussian distribution of the content's latent space\n",
        "        \"\"\"\n",
        "        #print(sentence_emb)\n",
        "        mu = self.content_mu(sentence_emb)\n",
        "        log_var = self.content_log_var(sentence_emb)\n",
        "\n",
        "        return mu, log_var\n",
        "\n",
        "    def get_style_emb(self, sentence_emb):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            sentence_emb: sentence embeddings of all the sentences in the batch, shape=(batch_size,2*gru_hidden_dim)\n",
        "        Returns:\n",
        "            mu: embedding of the mean of the Gaussian distribution of the style's latent space\n",
        "            log_var: embedding of the log of variance of the Gaussian distribution of the style's latent space\n",
        "        \"\"\"\n",
        "        #print(sentence_emb.size())\n",
        "        mu = self.style_mu(sentence_emb)\n",
        "        log_var = self.style_log_var(sentence_emb)\n",
        "\n",
        "        return mu, log_var\n",
        "\n",
        "    def sample_prior(self, mu, log_var):\n",
        "        \"\"\"\n",
        "        Returns samples drawn from the latent space constrained to\n",
        "        follow diagonal Gaussian\n",
        "        \"\"\"\n",
        "        print(\"Mu size: \", mu.size())\n",
        "        print(\"log_var size: \", log_var.size())\n",
        "        epsilon = torch.randn(mu.size(-1),device=mu.device)\n",
        "        #print(\"epsilon size: \", epsilon.size())\n",
        "        return mu + epsilon*torch.exp(log_var)\n",
        "\n",
        "    def update_average_style_emb(self, style_emb, style_labels):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            style_emb: batch of sampled style embeddings of the input sentences,shape = (batch_size,mconfig.style_hidden_dim)\n",
        "            style_labels: style labels of the corresponding input sentences,shape = (batch_size,2)\n",
        "        \"\"\"\n",
        "        # **key** will need to update to get average for every youtuber rather than just positive and negative\n",
        "        neg_style_label = torch.LongTensor([0, 1], device=style_emb.device)\n",
        "        # Iterate over the style labels\n",
        "        for idx, label in enumerate(style_labels):\n",
        "            # Calculate average for negative style\n",
        "            if neg_style_label.equal(label):\n",
        "                # Increment the counter for negative styles\n",
        "                self.num_neg_styles = self.num_neg_styles + 1\n",
        "                #print('author A increased count to:', self.num_neg_styles)\n",
        "                # Calculate a running average of the negative style embedding\n",
        "                self.avg_style_emb[0] = (\n",
        "                    (self.num_neg_styles-1) * self.avg_style_emb[0] + style_emb[idx])/self.num_neg_styles\n",
        "                #print('avg self embed author A:', self.avg_style_emb[0])\n",
        "            else:\n",
        "                # Increment the counter for positive styles\n",
        "                self.num_pos_styles = self.num_pos_styles + 1\n",
        "                # Calculate a running average of the positive style embedding\n",
        "                self.avg_style_emb[1] = (\n",
        "                    (self.num_pos_styles-1) * self.avg_style_emb[1] + style_emb[idx])/self.num_pos_styles\n",
        "        #print(self.avg_style_emb)\n",
        "\n",
        "    def get_content_disc_preds(self, style_emb):\n",
        "        \"\"\"\n",
        "        Returns predictions about the content using style embedding\n",
        "        as input\n",
        "        output shape : [batch_size,content_bow_dim]\n",
        "        \"\"\"\n",
        "        # predictions\n",
        "        # Note: detach the style embedding since when don't want the gradient to flow\n",
        "        #       all the way to the encoder. content_disc_loss is used only to change the\n",
        "        #       parameters of the discriminator network\n",
        "        preds = nn.Softmax(dim=1)(self.content_disc(\n",
        "            self.dropout(style_emb.detach())))\n",
        "\n",
        "        return preds\n",
        "\n",
        "    def get_content_disc_loss(self, content_disc_preds, content_bow):\n",
        "        \"\"\"\n",
        "        It essentially quantifies the amount of information about content\n",
        "        contained in the style space\n",
        "        Returns:\n",
        "        cross entropy loss of content discriminator\n",
        "        \"\"\"\n",
        "        # label smoothing\n",
        "        smoothed_content_bow = content_bow * \\\n",
        "            (1-mconfig.label_smoothing) + \\\n",
        "            mconfig.label_smoothing/mconfig.content_bow_dim\n",
        "        # calculate cross entropy loss\n",
        "        content_disc_loss = nn.BCELoss()(content_disc_preds, smoothed_content_bow)\n",
        "\n",
        "        return content_disc_loss\n",
        "\n",
        "    def get_style_disc_preds(self, content_emb):\n",
        "        \"\"\"\n",
        "        Returns predictions about style using content embeddings\n",
        "        as input\n",
        "        output shape: [batch_size,num_style]\n",
        "        \"\"\"\n",
        "        # predictions\n",
        "        # Note: detach the content embedding since when don't want the gradient to flow\n",
        "        #       all the way to the encoder. style_disc_loss is used only to change the\n",
        "        #       parameters of the discriminator network\n",
        "        preds = nn.Softmax(dim=1)(self.style_disc(\n",
        "            self.dropout(content_emb.detach())))\n",
        "\n",
        "        return preds\n",
        "\n",
        "    def get_style_disc_loss(self, style_disc_preds, style_labels):\n",
        "        \"\"\"\n",
        "        It essentially quantifies the amount of information about style\n",
        "        contained in the content space\n",
        "        Returns:\n",
        "        cross entropy loss of style discriminator\n",
        "        \"\"\"\n",
        "        # label smoothing\n",
        "        smoothed_style_labels = style_labels * \\\n",
        "            (1-mconfig.label_smoothing) + \\\n",
        "            mconfig.label_smoothing/mconfig.num_style\n",
        "        # calculate cross entropy loss\n",
        "\n",
        "        style_disc_loss = nn.BCELoss()(style_disc_preds, smoothed_style_labels)\n",
        "\n",
        "        return style_disc_loss\n",
        "\n",
        "    def get_entropy_loss(self, preds):\n",
        "        \"\"\"\n",
        "        Returns the entropy loss: negative of the entropy present in the\n",
        "        input distribution\n",
        "        \"\"\"\n",
        "        return torch.mean(torch.sum(preds * torch.log(preds + mconfig.epsilon), dim=1))\n",
        "\n",
        "    def get_content_mul_loss(self, content_emb, content_bow):\n",
        "        \"\"\"\n",
        "        This loss quantifies the amount of content information preserved\n",
        "        in the content space\n",
        "        Returns:\n",
        "        cross entropy loss of the content classifier\n",
        "        \"\"\"\n",
        "        # predictions\n",
        "        preds = nn.Softmax(dim=1)(\n",
        "            self.content_classifier(self.dropout(content_emb)))\n",
        "        # label smoothing\n",
        "        smoothed_content_bow = content_bow * \\\n",
        "            (1-mconfig.label_smoothing) + \\\n",
        "            mconfig.label_smoothing/mconfig.content_bow_dim\n",
        "        # calculate cross entropy loss\n",
        "        content_mul_loss = nn.BCELoss()(preds, smoothed_content_bow)\n",
        "\n",
        "        return content_mul_loss\n",
        "\n",
        "    def get_style_mul_loss(self, style_emb, style_labels):\n",
        "        \"\"\"\n",
        "        This loss quantifies the amount of style information preserved\n",
        "        in the style space\n",
        "        Returns:\n",
        "        cross entropy loss of the style classifier\n",
        "        \"\"\"\n",
        "        # predictions\n",
        "        preds = nn.Softmax(dim=1)(\n",
        "            self.style_classifier(self.dropout(style_emb)))\n",
        "        # label smoothing\n",
        "        smoothed_style_labels = style_labels * \\\n",
        "            (1-mconfig.label_smoothing) + \\\n",
        "            mconfig.label_smoothing/mconfig.num_style\n",
        "        # calculate cross entropy loss\n",
        "        style_mul_loss = nn.BCELoss()(preds, smoothed_style_labels)\n",
        "\n",
        "        return style_mul_loss\n",
        "\n",
        "    def get_annealed_weight(self, iteration, lambda_weight):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            iteration(int): Number of iterations compeleted till now\n",
        "            lambda_weight(float): KL penalty weight\n",
        "        Returns:\n",
        "            Annealed weight(float)\n",
        "        \"\"\"\n",
        "        return (math.tanh(\n",
        "            (iteration - mconfig.kl_anneal_iterations * 1.5) /\n",
        "            (mconfig.kl_anneal_iterations / 3))\n",
        "            + 1) * lambda_weight\n",
        "\n",
        "    def get_kl_loss(self, mu, log_var):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            mu: batch of means of the gaussian distribution followed by the latent variables\n",
        "            log_var: batch of log variances(log_var) of the gaussian distribution followed by the latent variables\n",
        "        Returns:\n",
        "            total loss(float)\n",
        "        \"\"\"\n",
        "        kl_loss = torch.mean((-0.5*torch.sum(1+log_var -\n",
        "                                             log_var.exp()-mu.pow(2), dim=1)))\n",
        "        return kl_loss\n",
        "\n",
        "    def generate_sentences(self, input_sentences, latent_emb, inference=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            latent_emb: generative embedding formed by the concatenation of sampled style and\n",
        "                       content latent embeddings, shape = (batch_size,mconfig.generative_emb_dim)\n",
        "            input_sentences: batch of token indices of input sentences, shape = (batch_size,max_seq_length)\n",
        "                            It is of type 'None' when the function is called in inference mode\n",
        "            inference: bool indicating whether train/inference mode\n",
        "        Returns:\n",
        "            output_sentences: batch of token indices or logits of generated sentences based on the\n",
        "            mode of operation.\n",
        "            modes:\n",
        "                train: shape = (max_seq_len,batch_size,vocab_size)\n",
        "                inference: shape = (max_seq_len,batch_size)\n",
        "        \"\"\"\n",
        "        # Training mode\n",
        "        if not inference:\n",
        "            # Prepend the input sentences with <sos> token\n",
        "            sos_token_tensor = torch.LongTensor(\n",
        "                [gconfig.predefined_word_index['<sos>']], device=input_sentences.device).unsqueeze(0).repeat(mconfig.batch_size, 1)\n",
        "            #print(sos_token_tensor.size(), 'sos token tensor')\n",
        "            #print(input_sentences.size(), 'input sentence size')\n",
        "            input_sentences = torch.cat(\n",
        "                (sos_token_tensor, input_sentences), dim=1)\n",
        "            #print(\"input sentences: \", input_sentences.size())\n",
        "            sentence_embs = self.dropout(self.embedding(input_sentences))\n",
        "            #print(\"sentence embs: \", sentence_embs.size())\n",
        "            # Make the latent embedding compatible for concatenation\n",
        "            # by repeating it for max_seq_len + 1(additional one bcoz <sos> tokens were added)\n",
        "            #print(\"latent_emb pre: \", latent_emb.size())\n",
        "            latent_emb = latent_emb.unsqueeze(1).repeat(\n",
        "                1, mconfig.max_seq_len+1, 1)\n",
        "            #print(\"latent_emb post: \", latent_emb.size())\n",
        "            gen_sent_embs = torch.cat(\n",
        "                (sentence_embs, latent_emb), dim=2)\n",
        "            # Delete latent embedding and sos token tensor to reduce memory usage\n",
        "            del latent_emb, sos_token_tensor\n",
        "            output_sentences = torch.zeros(\n",
        "                mconfig.max_seq_len, mconfig.batch_size, mconfig.vocab_size, device=input_sentences.device)\n",
        "            # initialize hidden state\n",
        "            hidden_states = torch.zeros(\n",
        "                mconfig.batch_size, mconfig.hidden_dim, device=input_sentences.device)\n",
        "            # generate sentences one word at a time in a loop\n",
        "            for idx in range(mconfig.max_seq_len):\n",
        "                # get words at the index idx from all the batches\n",
        "                words = gen_sent_embs[:, idx, :]\n",
        "                hidden_states = self.decoder(words, hidden_states)\n",
        "                # project over vocab space\n",
        "                next_word_logits = self.projector(hidden_states)\n",
        "                output_sentences[idx] = next_word_logits\n",
        "        # if inference mode is on\n",
        "        else:\n",
        "\n",
        "            sos_token_tensor = torch.LongTensor(\n",
        "                [gconfig.predefined_word_index['<sos>']], device=latent_emb.device) #.unsqueeze(0)\n",
        "            #print(\"sos_token_tensor : \", sos_token_tensor)\n",
        "            #print(\"sos_token_tensor unsqueezed: \", sos_token_tensor.unsqueeze(0))\n",
        "            word_emb = self.embedding(sos_token_tensor)\n",
        "            #print('unsqueezed embedding - ',self.embedding(sos_token_tensor.unsqueeze(0)))\n",
        "            #print('word-embedding:', word_emb)\n",
        "            #print(word_emb.size())\n",
        "            #print('latentembedding:', latent_emb)\n",
        "            #print(latent_emb.size())\n",
        "\n",
        "            hidden_states = torch.zeros(\n",
        "                1, mconfig.hidden_dim, device=latent_emb.device)\n",
        "            #print('hidden state:',hidden_states)\n",
        "            #print(hidden_states.size())\n",
        "            # Store output sentences\n",
        "            output_sentences = torch.zeros(\n",
        "                mconfig.max_seq_len, 1, device=latent_emb.device)\n",
        "            #print(\"output_sentence: \", output_sentences)\n",
        "            \n",
        "            #end_sentence_embedding = word_emb.repeat(mconfig.max_seq_length, 1)\n",
        "            with torch.no_grad():\n",
        "                # Greedily generate new words at a time \n",
        "                if mconfig.max_seq_len > latent_emb.size()[0]:\n",
        "                  max_index = latent_emb.size()[0]\n",
        "                else:\n",
        "                  max_index = mconfig.max_seq_len\n",
        "                for idx in range(max_index):\n",
        "                  #print(\"idx\", idx)\n",
        "                  #print(\"word emb size: \", word_emb.size())\n",
        "                  #print(\"latent emb size: \", latent_emb.size())\n",
        "                  #print(\"hidden_states size: \", hidden_states.size())\n",
        "                  #print(\"word embed\", word_emb.size())\n",
        "                  #print(\"Latent embed\", latent_emb.size())\n",
        "                  #word_and_latent_emb = torch.cat((end_sentence_embedding[1:idx, :], latent_emb[1:idx, :]), axis=1)\n",
        "                  #word_and_latent_emb = torch.cat((word_emb, latent_emb.mean(axis=0).view(1, -1)), axis=1)\n",
        "                  #print('word_emb  pre cat - ', word_emb)\n",
        "                  #print('word_emb size pre cat - ', word_emb.size())\n",
        "                  #print('latent_emb  pre cat - ', latent_emb)\n",
        "                  #print('latent_emb size  pre cat - ', latent_emb.size())\n",
        "                  #print('latent_emb[idx, :]  pre cat - ', latent_emb[idx,:].view(1, -1))\n",
        "                  word_and_latent_emb = torch.cat((word_emb, latent_emb[idx, :].view(1, -1)), axis=1)\n",
        "                  #print(\"word_and_latent_emb_grand size: \", word_and_latent_emb.size())\n",
        "                  hidden_states = self.decoder(word_and_latent_emb, hidden_states)   #Can we concatenate tensors to remember sequences?\n",
        "                  #print(\"hidden_states: \", hidden_states)\n",
        "                  #print(\"hidden_states size: \", hidden_states.size())\n",
        "                  #print(\"hidden states: \", hidden_states)\n",
        "                  next_word_probs = nn.Softmax(dim=1)(\n",
        "                      self.projector(hidden_states))\n",
        "                  #print('next_word probs', next_word_probs)\n",
        "                  #print('original format', next_word_probs.argmax(1))\n",
        "\n",
        "                  values, indices = torch.topk(next_word_probs,20, dim=1)\n",
        "                  #print(\"indices - \", indices)\n",
        "\n",
        "                  next_word = torch.tensor([indices.tolist()[0][2]], dtype=torch.int64) #next_word_probs.argmax(1)\n",
        "                  #print('new next word alt method',next_word)\n",
        "                  #print(vocab.lookup_token(next_word))\n",
        "                  #print(indices.tolist()[0][1])\n",
        "\n",
        "                  #values, indices = torch.topk(next_word_probs,2)\n",
        "                  #p = torch.no_grad(indices[1])\n",
        "                  #print(\"test p\", indices)\n",
        "                  #print('print 2nd most likely', next_word_probs.argmax(2))\n",
        "                  #output_sentences[idx] = next_word\n",
        "                  #word_emb = self.embedding(next_word)\n",
        "                  #print('output_sentences:', output_sentences)\n",
        "\n",
        "                  #next_word = next_word_probs.argmax(1)\n",
        "                  output_sentences[idx] = next_word\n",
        "                  word_emb = self.embedding(next_word)\n",
        "                  \n",
        "\n",
        "        return output_sentences\n",
        "\n",
        "    def get_recon_loss(self, output_logits, input_sentences):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            output_logits: logits of output sentences at each time step, shape = (max_seq_length,batch_size,vocab_size)\n",
        "            input_sentences: batch of token indices of input sentences, shape = (batch_size,max_seq_length)\n",
        "        Returns:\n",
        "            reconstruction loss calculated using cross entropy loss function\n",
        "        \"\"\"\n",
        "\n",
        "        loss = nn.CrossEntropyLoss(ignore_index=0)\n",
        "        recon_loss = loss(\n",
        "            output_logits.view(-1, mconfig.vocab_size), input_sentences.view(-1))\n",
        "\n",
        "        return recon_loss\n",
        "\n",
        "    def transfer_style(self, sequence, style):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            sequence : token indices of input sentence of shape = (random_seq_length,) \n",
        "            style: target style\n",
        "        Returns:\n",
        "            transfered_sentence: token indices of style transfered sentence, shape=(random_seq_length,)\n",
        "        \"\"\"\n",
        "        #print(\"Sequence: \", sequence)\n",
        "        #print(\"Style: \", style)\n",
        "\n",
        "        # pack the sequences to reduce unnecessary computations\n",
        "        # It requires the sentences to be sorted in descending order to take\n",
        "        # full advantage\n",
        "        seq_len = torch.tensor(len(sequence)).view(1)\n",
        "        seq_len = seq_len.cpu()\n",
        "        print(\"seq_len: \", seq_len)\n",
        "        #print(\"output: \", output.size())\n",
        "        #sentence_emb = output[torch.arange(output.size(0)), seq_lengths-1]\n",
        "        temp = [index if index != -1 else torch.tensor(vocab.vocab['<unk>']) for index in sequence] #fix unk token\n",
        "        #print(temp)\n",
        "        #[temp.append(torch.tensor(vocab.vocab['<pad>'])) for i in range(20 - len(temp))]  #add padding\n",
        "        temp = torch.tensor(temp) \n",
        "        temp = temp.cpu()\n",
        "        embedded_seq = self.embedding(temp)\n",
        "        embedded_seq = embedded_seq.view(1, len(sequence), -1)\n",
        "        #print('embedding size: ', embedded_seq.size())\n",
        "        print('embedding sequence size: ', embedded_seq.size())\n",
        "        #print(seq_len.size())\n",
        "\n",
        "        packed_seqs = pack_padded_sequence(\n",
        "            embedded_seq, lengths=seq_len, batch_first=True)\n",
        "        print('packed_output: ', packed_seqs.data.size()) \n",
        "        packed_output, (_) = self.encoder(packed_seqs)\n",
        "        print('packed_output: ', packed_output.data.size()) \n",
        "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
        "        #output, final_hidden_state = self.encoder(embedded_seq)\n",
        "        print('output: ', output.size()) # Needs to be 1 by (size sentence) by 512\n",
        "        print(output.size(0))\n",
        "        print(seq_len-1)\n",
        "        sentence_emb = output[torch.arange(output.size(0)), seq_len-1]\n",
        "        print(\"sentence_embedding: \", sentence_emb.size())\n",
        "        #print(sentence_emb)\n",
        "        # get content embeddings\n",
        "        # Note that we need not calculate style embeddings since we\n",
        "        # use the target style embedding\n",
        "        content_emb_mu, content_emb_log_var = self.get_content_emb(\n",
        "            sentence_emb)\n",
        "        #print(\"content_emb_mu: \", content_emb_mu.size())\n",
        "        #print(\"content_emb_log_var: \", content_emb_log_var.size())\n",
        "\n",
        "        # sample content embeddings latent space\n",
        "        sampled_content_emb = self.sample_prior(\n",
        "            content_emb_mu, content_emb_log_var)\n",
        "        #print('Content Embed Mu - ', content_emb_mu)\n",
        "        print('Sampled Content Emb: ', sampled_content_emb.size())\n",
        "\n",
        "\n",
        "        # Get the approximate estimate of the target style embedding\n",
        "        target_style_emb = self.avg_style_emb[style]\n",
        "        #print('Target Style Emb - ', target_style_emb)\n",
        "        print(\"Target Style Embedding: \")\n",
        "        #print(target_style_emb)\n",
        "        print(target_style_emb.size())\n",
        "        x = target_style_emb.repeat(sampled_content_emb.size()[0], 1)\n",
        "        print(\"X: \")\n",
        "        #print(x)\n",
        "        print(x.size())\n",
        "\n",
        "        # Generative embedding\n",
        "        #print(\"Sampled Content Embedding: \")\n",
        "        #print(sampled_content_emb)\n",
        "        #print(sampled_content_emb.size())\n",
        "\n",
        "        #chosen_sampled_content_emb = sampled_content_emb[1, :]\n",
        "        #print('Chosen Sample Content Emb - ', chosen_sampled_content_emb)\n",
        "        #print(\"Chosen_sampled_content_emb: \", chosen_sampled_content_emb)\n",
        "        #print(chosen_sampled_content_emb.size())\n",
        "        generative_emb = torch.cat(\n",
        "            (x, sampled_content_emb), axis=1)\n",
        "        #print('Generative Emb: ', generative_emb)\n",
        "        print(\"Content embedding: \", sampled_content_emb.size())\n",
        "        \n",
        "        # Generate the style transfered sentences\n",
        "        #transfered_sentence = self.generate_sentences(\n",
        "        #    input_sentencs=None, latent_emb=generative_emb, inference=True)\n",
        "        transfered_sentence = self.generate_sentences(\n",
        "            input_sentences=None, latent_emb=generative_emb, inference=True)\n",
        "\n",
        "        return transfered_sentence.view(-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOM-2csmk8AL"
      },
      "source": [
        "Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 693
        },
        "id": "ye2y0NTRkl00",
        "outputId": "caf8c7e6-68ae-461d-c804-e2912c0be8f7",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(44120, 20)\n",
            "Training started!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch:   0%|          | 0/20 [00:00<?, ?it/s]\n",
            "  0%|          | 0/885 [00:00<?, ?it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pre encoder embedded_seqs size:  torch.Size([128, 20, 64])\n",
            "seq_lengths:  torch.Size([128])\n",
            "packed seqs:  torch.Size([2560, 64])\n",
            "packed_output:  torch.Size([2560, 512])\n",
            "output:  torch.Size([128, 20, 512])\n",
            "after encoder sentence_emb:  torch.Size([128, 512])\n",
            "Mu size:  torch.Size([128, 128])\n",
            "log_var size:  torch.Size([128, 128])\n",
            "Mu size:  torch.Size([128, 8])\n",
            "log_var size:  torch.Size([128, 8])\n",
            "input sentences:  torch.Size([128, 21])\n",
            "latent_emb pre:  torch.Size([128, 136])\n",
            "latent_emb post:  torch.Size([128, 21, 136])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/885 [00:02<?, ?it/s]\n",
            "Epoch:   0%|          | 0/20 [00:02<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-a15e06a461d2>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;31m#=============== Update VAE and classifier parameters ===============#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0mvae_and_cls_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0mvae_and_cls_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mvae_and_cls_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#@title Train\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "#from linguistic_style_transfer_pytorch.config import GeneralConfig, ModelConfig\n",
        "#from linguistic_style_transfer_pytorch.data_loader import TextDataset\n",
        "#from linguistic_style_transfer_pytorch.model import AdversarialVAE\n",
        "\n",
        "from tqdm import tqdm, trange\n",
        "import os\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "use_cuda = True if torch.cuda.is_available() else False\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    mconfig = ModelConfig()\n",
        "    gconfig = GeneralConfig()\n",
        "    weights = torch.FloatTensor(np.load('/content/drive/MyDrive/Style-Transfer/Data/embedding_matrix.npy')) #/content/linguistic-style-transfer-pytorch/linguistic_style_transfer_pytorch/data/word_embeddings.npy\n",
        "    \n",
        "    model = AdversarialVAE(weight=weights)\n",
        "    #checkpoints = torch.load('/content/drive/MyDrive/Style-Transfer/Models/checkpoints/model_epoch_7.pt')  #last epoch trained\n",
        "    #model.load_state_dict(checkpoints)\n",
        "    #with open('/content/drive/MyDrive/Style-Transfer/Models/checkpoints/avg_style_emb.pkl', 'rb') as f:\n",
        "      #avg_style_embeddings = pickle.load(f)\n",
        "    #model.avg_style_emb = avg_style_embeddings\n",
        "\n",
        "    if use_cuda:\n",
        "        model = model.cuda()\n",
        "\n",
        "    #=============== Define dataloader ================#\n",
        "    train_dataset = TextDataset(mode='train')\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=mconfig.batch_size)\n",
        "    content_discriminator_params, style_discriminator_params, vae_and_classifier_params = model.get_params()\n",
        "    #============== Define optimizers ================#\n",
        "    # content discriminator/adversary optimizer\n",
        "    content_disc_opt = torch.optim.RMSprop(\n",
        "        content_discriminator_params, lr=mconfig.content_adversary_lr)\n",
        "    # style discriminaot/adversary optimizer\n",
        "    style_disc_opt = torch.optim.RMSprop(\n",
        "        style_discriminator_params, lr=mconfig.style_adversary_lr)\n",
        "    # autoencoder and classifiers optimizer\n",
        "    vae_and_cls_opt = torch.optim.Adam(\n",
        "        vae_and_classifier_params, lr=mconfig.autoencoder_lr)\n",
        "    print(\"Training started!\")\n",
        "    for epoch in trange(mconfig.epochs, desc=\"Epoch\"):\n",
        "        count = 0\n",
        "        pass_count = 0\n",
        "        for iteration, batch in enumerate(tqdm(train_dataloader)):\n",
        "            count += 1\n",
        "            #print(count)\n",
        "            # unpacking\n",
        "            if count == 344: break\n",
        "            sequences, seq_lens, labels, bow_rep = batch\n",
        "            if use_cuda:\n",
        "              sequences = sequences.cuda()\n",
        "              seq_lens = seq_lens.cuda()\n",
        "              labels = labels.cuda()\n",
        "              bow_rep = bow_rep.cuda()\n",
        "            #####\n",
        "            #print(\"\\n\\n\\nSequences size: \", sequences.size())\n",
        "            #print(\"Seq_lens: \", seq_lens.size())\n",
        "            #print(\"Seq_lens: \", seq_lens.squeeze(1).size())\n",
        "            #print(\"Labels: \", labels.size())\n",
        "            #print(\"Bow representation: \", bow_rep.size(), \"\\n\\n\\n\")\n",
        "            #####\n",
        "            content_disc_loss, style_disc_loss, vae_and_cls_loss = model(\n",
        "                sequences, seq_lens.squeeze(1), labels, bow_rep, iteration+1, epoch == mconfig.epochs-1)\n",
        "\n",
        "            #============== Update Adversary/Discriminator parameters ===========#\n",
        "            # update content discriminator parametes\n",
        "            # we need to retain the computation graph so that discriminator predictions are\n",
        "            # not freed as we need them to calculate entropy.\n",
        "            # Note that even detaching the discriminator branch won't help us since it\n",
        "            # will be freed and delete all the intermediary values(predictions, in our case).\n",
        "            # Hence, with no access to this branch we can't backprop the entropy loss\n",
        "            content_disc_loss.backward(retain_graph=True)\n",
        "            content_disc_opt.step()\n",
        "            content_disc_opt.zero_grad()\n",
        "\n",
        "            # update style discriminator parameters\n",
        "            style_disc_loss.backward(retain_graph=True)\n",
        "            style_disc_opt.step()\n",
        "            style_disc_opt.zero_grad()\n",
        "\n",
        "            #=============== Update VAE and classifier parameters ===============#\n",
        "            vae_and_cls_loss.backward()\n",
        "            vae_and_cls_opt.step()\n",
        "            vae_and_cls_opt.zero_grad()\n",
        "\n",
        "        print(\"Saving states\")\n",
        "        #================ Saving states ==========================#\n",
        "        if not os.path.exists(gconfig.model_save_path):\n",
        "            os.mkdir(gconfig.model_save_path)\n",
        "        # save model state\n",
        "        torch.save(model.state_dict(), gconfig.model_save_path +\n",
        "                   f'/model_epoch_{epoch + 1}.pt')\n",
        "        # save optimizers states\n",
        "        torch.save({'content_disc': content_disc_opt.state_dict(\n",
        "        ), 'style_disc': style_disc_opt.state_dict(), 'vae_and_cls': vae_and_cls_opt.state_dict()}, gconfig.model_save_path+f'/opt_epoch_{epoch + 1}.pt')\n",
        "        # Save approximate estimate of different style embeddings after the last epoch\n",
        "        pickle.dump(model.avg_style_emb, open(gconfig.avg_style_emb_path, 'wb'))\n",
        "        break\n",
        "    print(\"Training completed!!!\")\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Open Previous Models\n",
        "#Load from Style-Transfer/Models/checkpoints\n",
        "import torch\n",
        "model_checkpoint = torch.load('/content/drive/MyDrive/Style-Transfer/Models/checkpoints/model_epoch_6.pt') # This is the actual AdversarialVAE model\n",
        "\n",
        "# We should probably change the name of end models after we go all the way through -- name them by their date of creation"
      ],
      "metadata": {
        "id": "9g3I2KAsjDo4",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5x2ZUDry5Pl",
        "outputId": "a9c0352b-251c-4b9d-e53f-772133f0bcc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "seq_len:  tensor([12])\n",
            "embedding sequence size:  torch.Size([1, 12, 64])\n",
            "packed_output:  torch.Size([12, 64])\n",
            "packed_output:  torch.Size([12, 512])\n",
            "output:  torch.Size([1, 12, 512])\n",
            "1\n",
            "tensor([11])\n",
            "sentence_embedding:  torch.Size([1, 512])\n",
            "Mu size:  torch.Size([1, 128])\n",
            "log_var size:  torch.Size([1, 128])\n",
            "Sampled Content Emb:  torch.Size([1, 128])\n",
            "Target Style Embedding: \n",
            "torch.Size([8])\n",
            "X: \n",
            "torch.Size([1, 8])\n",
            "Content embedding:  torch.Size([1, 128])\n",
            "tensor([5898.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
            "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.])\n",
            "Style transfered sentence: \n",
            "\n",
            "center. <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n"
          ]
        }
      ],
      "source": [
        "#@title Generate\n",
        "import torch\n",
        "import os\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pickle\n",
        "import sys\n",
        "#sys.path.append('/content/linguistic-style-transfer-pytorch/')\n",
        "#from config import GeneralConfig\n",
        "#from model import AdversarialVAE\n",
        "\n",
        "gconfig = GeneralConfig()\n",
        "\n",
        "# load word embeddings\n",
        "weights = torch.FloatTensor(np.load('/content/drive/MyDrive/Style-Transfer/Data/embedding_matrix.npy'))\n",
        "# load checkpoint\n",
        "path_load_checkpoints = ''\n",
        "model_checkpoint = torch.load('/content/drive/MyDrive/Style-Transfer/Models/checkpoints/model_epoch_6.pt') # This is the actual AdversarialVAE model\n",
        "# Load model\n",
        "model = AdversarialVAE(weight=weights)\n",
        "model.load_state_dict(model_checkpoint)\n",
        "model.eval()\n",
        "# Load average style embeddings\n",
        "with open('/content/drive/MyDrive/Style-Transfer/Models/checkpoints/avg_style_emb.pkl', 'rb') as f:\n",
        "    avg_style_embeddings = pickle.load(f)\n",
        "\n",
        "#avg_style_embeddings = pickle.load('/content/drive/MyDrive/Style-Transfer/Models/checkpoints/avg_style_emb.pkl')\n",
        "# set avg_style_emb attribute of the model\n",
        "model.avg_style_emb = avg_style_embeddings\n",
        "# load vocab\n",
        "vocab = torch.load(gconfig.vocab_file_path)\n",
        "label2index = {'Kings and Generals': 0, '3Blue1Brown': 1} # **key** will need to change to the youtubers\n",
        "\n",
        "\n",
        "# Read input sentence\n",
        "#source_sentence = input(\"Enter the source sentence\\n\")\n",
        "#target_style = input(\"Enter the target style: Kings and Generals or 3Blue1Brown\\n\")  \n",
        "source_sentence =  '<sos> the integral of the function is a sum of infinite distances' #'test test hello '#\"kings and generals channel make videos about this topic\"  #'Try this sentence now, I bet its really hard'  \n",
        "target_style_id = 1\n",
        "\n",
        "# Get token ids\n",
        "token_ids = [vocab[word] for word in source_sentence.split()]\n",
        "token_ids = torch.LongTensor(token_ids)\n",
        "#target_style_id = torch.LongTensor(label2index[target_style])\n",
        "# Get transfered sentence token ids\n",
        "#print(token_ids)\n",
        "target_tokenids = model.transfer_style(token_ids, target_style_id)\n",
        "print(target_tokenids)\n",
        "target_sentence = \" \".join([vocab.lookup_token(idx) for idx in target_tokenids])\n",
        "print(\"Style transfered sentence: \\n\\n{}\".format(target_sentence))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSordn06g01t"
      },
      "outputs": [],
      "source": [
        "<pad> <sos> reducing acts 25 brains , saints saints velocity velocity 2/16th sick kütahı continuously resentment helped rapid harsh etc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9eJOhzp7SmZL"
      },
      "outputs": [],
      "source": [
        "<pad> <sos> reducing acts , 25 saints unchallenged velocity brains resentment 2/16th continuously creature 4,000 kütahı kütahı etc sick independently"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word = ''\n",
        "#a = [word + '<pad>' for i in range(20)]\n",
        "\n",
        "for i in range(20):\n",
        "  word += ' supporting'"
      ],
      "metadata": {
        "id": "T_VRTQD9zA-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "jEbdzmUhzgNc",
        "outputId": "9705cf2a-a84b-418e-c2bd-f26e5a93f2ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' supporting supporting supporting supporting supporting supporting supporting supporting supporting supporting supporting supporting supporting supporting supporting supporting supporting supporting supporting supporting'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m_prop = 4/17"
      ],
      "metadata": {
        "id": "QFplANL8BNHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "7*60"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wTKwC7tBQT7",
        "outputId": "c6d2e007-16e8-4495-97fa-5b11058f7b9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "420"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m_prop*420 #same as ben, ideally ben has little more to talk about model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdvIvdEIBVIr",
        "outputId": "0a6ea8b7-2d95-4803-95ce-d812212fe669"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "98.82352941176471"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jo_prop = 3/17\n",
        "jo_prop*420"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJuEHFSuBeP9",
        "outputId": "3325a51c-a4da-4982-8190-452377a30dc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "74.11764705882354"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ja_prop = 6/17\n",
        "ja_prop*420"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEz985_sB9I8",
        "outputId": "24bdc23a-a7dd-43d0-d29b-1a70d229a9a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "148.23529411764707"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_KfAXWQtCMcn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
