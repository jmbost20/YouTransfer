# YouTransfer: YouTube Transcript Generation with AdversarialVAE

This project was developed in tandem by four students at the University of Wisconsin-Madison to contribute to the topic of text style transfer. The project utilizes Google Colab and Jupyter Notebook to pull transcripts from different YouTube videos and then applies Natural Language Processing (NLP) techniques to generate text using an AdversarialVAE (Variational Autoencoder) model with the goal to generate desired text from any YouTuber.

## Table of Contents

- [Introduction](#introduction)
- [Installation](#installation)
- [Usage](#usage)
- [Development](#development)
  - [Data Collection](#data-collection)
  - [Preprocessing](#preprocessing)
  - [AdversarialVAE Model](#adversarialvae-model)
  - [Results](#results)
- [Contributing](#contributing)

## Introduction

YouTube Transcript Generation with AdversarialVAE is a project that combines the power of NLP and generative models to automatically generate text based on YouTube video content. By leveraging the transcripts available for YouTube videos, this project extracts textual information and employs an AdversarialVAE model to generate realistic and coherent text.

## Packages

This project can be run in Google Colab as it was developed, or it can be ran in a local environment as a .ipynb or converted to .py.

To run this project, you need to have the following dependencies installed:

- youtube_transcript_api
- scrapetube
- pandas
- nltk
- torch
- gensim
- numpy
- Python 3.x
- TensorFlow
- PyTorch
- Transformers

## Usage

To use with Google Colab: 

Mount your Google Drive by running the code cell containing drive.mount('/content/drive'). This step allows access to the required files and storage for saving the processed data.

Update the channels list variable with the names of the YouTube channels for which you want to clean and prepare the transcript data.

Execute the cells in the notebook one by one. The notebook will perform the following tasks:

1. Load the transcript data for the specified YouTube channels.
2. Perform cleaning and tokenization on the transcript text.
3. Format the transcript data into appropriate inputs for further processing. Save the formatted transcript data, vocabulary mappings, and labels to the specified locations in Google Drive.
4. After executing the notebook, you will have the cleaned and formatted transcript data, vocabulary mappings, and labels saved in the specified locations on your Google Drive.


# Development

The next sections highlight the steps of development this project undertook.

## Data Collection

The first step in this project is to collect transcripts from various YouTube videos. The YouTube Data API can be used to retrieve video information, including transcripts, using appropriate API calls. You need to provide API keys and specify the desired YouTube video IDs to fetch the transcripts; however, this project utilized "youtube_transcript_api" and "scrapetube" libraries with the purpose of bypassing expensive YouTube API calls. 

The code initializes a DataFrame for storing video information, including raw and full transcripts. It loops through the video list, retrieves the raw transcripts using the "YouTubeTranscriptApi," and populates the DataFrame accordingly. If a transcript is missing or fails to retrieve, it marks it as False in the DataFrame. Finally, the DataFrame is saved as a CSV file, which can be utilized downstream.

## Preprocessing

Once the transcripts are collected, preprocessing is performed to clean the text data. Preprocessing steps include removing special characters, lowercasing, tokenization, and removing stopwords. These steps help in preparing the data for the AdversarialVAE model. 

Specifically, these steps utilizes Natural Language Processing (NLP) techniques to clean and prepare transcript data. It performs text cleaning by removing unwanted characters and converting the text to lowercase. The notebook then applies tokenization using NLTK library for sentence and word tokenization, breaking the text into sentences and further splitting them into words. The tokenized data is formatted by dividing it into smaller chunks based on calculated parameters, ensuring manageable input sizes for subsequent processing. A vocabulary mapping is generated by counting word occurrences and creating an ordered dictionary for efficient encoding and decoding. The cleaned and formatted transcript data, vocabulary mappings, and labels are saved for further analysis or machine learning tasks. Overall, this notebook demonstrates how NLP techniques can be used to preprocess and prepare transcript data for various downstream applications.

Finally, word2vec is used to processes the corpus data by converting each sublist into a list and appending a special token '<unk>' to indicate unknown words. It specifies the embedding size and the path to save the trained Word2Vec model. The Word2Vec model is trained on the processed corpus data using the specified parameters, and the trained model is saved to the specified file path. The code then loads the vocabulary index from a saved file and retrieves the embedding vectors for each word in the vocabulary. The code also demonstrates loading and processing additional data files, such as labels and bag-of-words (BoW) filtered vocabulary indices, by loading and printing their contents.


## Model AdversarialVAE 

The AdversarialVAE model is a generative model that combines the principles of variational autoencoders and adversarial training. It consists of an encoder, decoder, and discriminator. The encoder encodes input text into a latent space, the decoder reconstructs the input text from the latent space, and the discriminator tries to distinguish between real and generated text. The model is trained using a combination of reconstruction loss, KL divergence, and adversarial loss.

## Results

The results are best exemplified in the "Presentation_YouTransfer.pdf", but while there were some shortcomings with the modelling, the project was promising overall. The project will next undergo updates and iterations to the modelling steps as well as the addition of a UI to enable ease of use for users.

## Contributing

Contributions to this project are welcome. If you have any suggestions, bug fixes, or additional features, feel free to open a pull request. Please ensure that your contributions align with the project's coding style and guidelines.


