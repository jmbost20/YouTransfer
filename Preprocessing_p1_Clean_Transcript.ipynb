{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmbost20/YouTransfer/blob/main/Clean_Prepare_Transcript.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDpgnYX5ze05",
        "outputId": "d99aed34-62b9-423e-ba2a-1be3309caf85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#For handling\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "#For tokenizing \n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "#For vocab building and tensor operations\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "torch.manual_seed(42)\n",
        "\n",
        "import json\n",
        "\n",
        "from collections import Counter, OrderedDict\n",
        "from torchtext.vocab import vocab\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWralwVb0ofB"
      },
      "outputs": [],
      "source": [
        "def clean_words_string(id, df):\n",
        "  words_string = df.loc[id]['Full_Transcript']\n",
        "  words_string = words_string.replace('\\\\n',' ').replace('\\\\xa0', '').replace('\\'','').replace('[Music]', '').replace('[', '').replace(']', '').replace(',,',',').lower()\n",
        "  return words_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsczPK5kQm3y"
      },
      "outputs": [],
      "source": [
        "def pull_draw(threshold,minimum, mean_len):\n",
        "  #grab initial draw with mean equal to either mean of \n",
        "  draw = np.random.poisson(min(mean_len, 15))\n",
        "  while(draw > threshold and draw < minimum):\n",
        "      draw = np.random.poisson(mean_len)\n",
        "  return draw\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdehj-NRKxuL"
      },
      "outputs": [],
      "source": [
        "#words is a list of lists containing word tokens from each data entry\n",
        "\n",
        "def generate_formatted_inputs(words):\n",
        "  #Calc lengths to check sizes\n",
        "  t = [len(i) for i in words]\n",
        "  mean_len = sum(t)/len(words)\n",
        "  minimum = max(min(t),15)  #Want a reasonably high lower bound\n",
        "  del t\n",
        "\n",
        "  #Set hyper-parameters\n",
        "  stopping_threshold_size = minimum\n",
        "  upper_bound_input_size = 75\n",
        "  \n",
        "  new_list = []\n",
        "  for i in words:\n",
        "    #Calc number of word tokens in the ith entry\n",
        "    length = len(i)\n",
        "\n",
        "    #Check if entry is too long\n",
        "    if length > upper_bound_input_size:\n",
        "      #Draw initial value to determine first index cutoff\n",
        "      draw = pull_draw(upper_bound_input_size,minimum,mean_len)\n",
        "\n",
        "      #define sum of draws to serve as rolling lower index\n",
        "      draw_sum = draw\n",
        "\n",
        "      #append first formatted entry to list of all entries\n",
        "      new_list.append(i[0:draw])\n",
        "\n",
        "      #compute how many tokens remain\n",
        "      d = length - draw_sum\n",
        "\n",
        "      while(d > stopping_threshold_size):\n",
        "        #draw with a bound dependent on amount of space left \n",
        "        draw = pull_draw(min(upper_bound_input_size,d),minimum,mean_len)\n",
        "\n",
        "        #append new entry\n",
        "        new_list.append(i[draw_sum:draw_sum+draw])\n",
        "        \n",
        "        #update conditions for index and loop\n",
        "        draw_sum += draw\n",
        "        d -= draw\n",
        "      #append remaining as final list from this entry\n",
        "      new_list.append(i[draw_sum:length-1])\n",
        "    else:\n",
        "      #append formatted entry\n",
        "      new_list.append(i)\n",
        "  \n",
        "  return new_list\n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#words is a list of lists containing word tokens from each data entry\n",
        "\n",
        "def generate_formatted_inputs_NEW(words):\n",
        "  #Calc lengths to check sizes\n",
        "  t = [len(i) for i in words]\n",
        "  mean_len = sum(t)/len(words)\n",
        "  minimum = max(min(t),10)  #Want a reasonably high lower bound  #Chnage from 15 to 10\n",
        "  del t\n",
        "\n",
        "  #Set hyper-parameters\n",
        "  stopping_threshold_size = minimum\n",
        "  upper_bound_input_size = 20  #Change from 75 to 20\n",
        "  \n",
        "  new_list = []\n",
        "  for i in words:\n",
        "    #Calc number of word tokens in the ith entry\n",
        "    length = len(i)\n",
        "\n",
        "    #Check if entry is too long\n",
        "    if length > upper_bound_input_size:\n",
        "      #Draw initial value to determine first index cutoff\n",
        "      draw = pull_draw(upper_bound_input_size,minimum,mean_len)\n",
        "\n",
        "      #define sum of draws to serve as rolling lower index\n",
        "      draw_sum = draw\n",
        "\n",
        "      #append first formatted entry to list of all entries\n",
        "      new_list.append(np.array(i[0:draw]))\n",
        "\n",
        "      #compute how many tokens remain\n",
        "      d = length - draw_sum\n",
        "\n",
        "      while(d > stopping_threshold_size):\n",
        "        #draw with a bound dependent on amount of space left \n",
        "        draw = pull_draw(min(upper_bound_input_size,d),minimum,mean_len)\n",
        "\n",
        "        #append new entry\n",
        "        new_list.append(np.array(i[draw_sum:draw_sum+draw]))\n",
        "        \n",
        "        #update conditions for index and loop\n",
        "        draw_sum += draw\n",
        "        d -= draw\n",
        "      #append remaining as final list from this entry\n",
        "      new_list.append(np.array(i[draw_sum:length-1]))\n",
        "    else:\n",
        "      #append formatted entry\n",
        "      new_list.append(np.array(i))\n",
        "  \n",
        "  return np.array(new_list)\n",
        "\n",
        "   "
      ],
      "metadata": {
        "id": "RYGBGUs0B3n9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPbQXqPXDK6m"
      },
      "outputs": [],
      "source": [
        "def format_transcript_data(Channel_Name):\n",
        "\n",
        "  #Note this may need to change depending on the machine, ideally we will have a shared folder that is accessible to all using same path\n",
        "  file_path = f'/content/drive/MyDrive/Style-Transfer/Youtuber-Transcripts/{Channel_Name}.csv'   \n",
        "  df = pd.read_csv(file_path, index_col = 0)\n",
        "\n",
        "  max_dim_size = 20 #Change from 75 to 20\n",
        "\n",
        "  all_transcript_inputs = np.array([])\n",
        "  num_videos = len(df.index) # df.index\n",
        "  ### Use to get analytics on tokens\n",
        "  print(f\"total num of videos for {Channel_Name}: \",num_videos) # number of the videos\n",
        "\n",
        "  for id in df.index:\n",
        "    #Get transcript as single string and perform text cleaning\n",
        "    words_string = clean_words_string(id, df)\n",
        "\n",
        "    #Break string into sequence of sentence tokens: NLTK sent_tokenizer preferred\n",
        "    list_nltk_sentence_tokenizer = sent_tokenize(words_string)\n",
        "\n",
        "    #Break every sentence into word tokens\n",
        "    word_tokenization_list = [word_tokenize(i) for i in list_nltk_sentence_tokenizer]\n",
        "    \n",
        "    formatted_input_list = generate_formatted_inputs_NEW(word_tokenization_list)\n",
        "    formatted_input_array = np.array(formatted_input_list)\n",
        "\n",
        "    clean_input_array = formatted_input_array.flatten('C')\n",
        "    # print(\"TOTAL Sentences in transcript\", len(clean_input_array)) \n",
        "    all_transcript_inputs = np.hstack((all_transcript_inputs, clean_input_array))\n",
        "\n",
        "    # if len(clean_input_array) <10:\n",
        "    #   print(id)\n",
        "    #   print([len(i) for i in clean_input_array])\n",
        "  result = [[j[i] if i < len(j) else '<pad>' for i in range(max_dim_size-1)] for j in all_transcript_inputs]\n",
        "  [i.insert(0, \"<sos>\") for i in result]\n",
        "  #print(result[0])\n",
        "  num_tokens= len(result)\n",
        "  print(num_tokens) #sentence tokens\n",
        "\n",
        "  print(num_tokens/ num_videos)\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3smfRDfszGuk"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def make_vocabulary_map(loaded_corpus):\n",
        "  filepath_bow = '/content/drive/MyDrive/Style-Transfer/Data/BoW_file.json'\n",
        "\n",
        "  #Set size limit on vocab list\n",
        "  slice_index = 9999\n",
        "  temp_list = loaded_corpus\n",
        "  #flat_list = [item for youtuber_list in temp_list for sublist in youtuber_list for item in sublist]  #Assume form [Youtuber[sentence[word]]]\n",
        "  flat_list = [item for sentence in temp_list for item in sentence]\n",
        "  counter = Counter(flat_list)\n",
        "  sliced_sorted_by_freq_tuples = sorted(counter.items(), key=lambda x: x[1], reverse=True)[0:slice_index]\n",
        "\n",
        "  ordered_dict = OrderedDict(sliced_sorted_by_freq_tuples)\n",
        "\n",
        "  with open(filepath_bow, \"w\") as outfile:\n",
        "      outfile.write(json.dumps(ordered_dict))\n",
        "\n",
        "  unk_token = '<unk>'\n",
        "  #sos_token = '<sos>'\n",
        "  #pad_token = '<pad>'\n",
        "  default_index = -1\n",
        "  vocab_mapping = vocab(ordered_dict, specials=[unk_token])\n",
        "  #vocab_mapping = vocab(ordered_dict)\n",
        "  vocab_mapping.set_default_index(default_index)\n",
        "  return ordered_dict, vocab_mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjbAcxcRzKzF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a853162b-0d26-436c-d55b-def91b16ec12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total num of videos for Kings and Generals:  735\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-1792a47de500>:49: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  return np.array(new_list)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "91121\n",
            "123.97414965986394\n",
            "total num of videos for 3Blue1Brown:  119\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-1792a47de500>:49: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  return np.array(new_list)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22060\n",
            "185.3781512605042\n"
          ]
        }
      ],
      "source": [
        " channels = ['Kings and Generals','3Blue1Brown'] ### Add\n",
        "# #channels= ['3Blue1Brown']\n",
        "# #/content/drive/MyDrive/Style-Transfer/Youtuber-Transcripts/Kings and Generals.csv\n",
        "corpus = [format_transcript_data(i) for i in channels]\n",
        "lengths = [len(i) for i in corpus]\n",
        "corpus = [temp for sublist in corpus for temp in sublist]\n",
        "#[i.insert(0,\"<sos>\") for i in corpus]\n",
        "#ordered_dict, vocabulary_map = make_vocabulary_map(corpus)\n",
        "# #\n",
        "# corpus_path = '/content/drive/MyDrive/Style-Transfer/Data/Corpus.npy'\n",
        "# np.save(corpus_path,np.array(corpus), allow_pickle=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "channels =[\"ElectroBOOM\", \"Historia Civilis\", \"Kings and Generals\", \"PBS Eons\", \"Moth Light Media\", \"3Blue1Brown\", \"Dr Dray\"]\n",
        "#channels = ['Kings and Generals','3Blue1Brown'] ### Add\n",
        "#channels= ['3Blue1Brown']\n",
        "#/content/drive/MyDrive/Style-Transfer/Youtuber-Transcripts/Kings and Generals.csv\n",
        "corpus = [format_transcript_data(i) for i in channels]\n",
        "lengths = [len(i) for i in corpus] # \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9QuqKSqyCCm",
        "outputId": "d49f1767-6740-4c09-cffc-d4547a9ceb81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total num of videos for ElectroBOOM:  198\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-1792a47de500>:49: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  return np.array(new_list)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11788\n",
            "59.535353535353536\n",
            "total num of videos for Historia Civilis:  83\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-1792a47de500>:49: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  return np.array(new_list)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16561\n",
            "199.53012048192772\n",
            "total num of videos for Kings and Generals:  735\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-1792a47de500>:49: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  return np.array(new_list)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "91230\n",
            "124.12244897959184\n",
            "total num of videos for PBS Eons:  225\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-1792a47de500>:49: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  return np.array(new_list)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13649\n",
            "60.66222222222222\n",
            "total num of videos for Moth Light Media:  101\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-1792a47de500>:49: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  return np.array(new_list)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "199\n",
            "1.9702970297029703\n",
            "total num of videos for 3Blue1Brown:  119\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-1792a47de500>:49: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  return np.array(new_list)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22190\n",
            "186.47058823529412\n",
            "total num of videos for Dr Dray:  2428\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-1792a47de500>:49: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  return np.array(new_list)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "676123\n",
            "278.46911037891266\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_path = '/content/drive/MyDrive/Style-Transfer/Data/Vocabulary_obj.pth'\n",
        "torch.save(vocabulary_map, vocab_path)  #Save Vocab Index\n",
        "\n",
        "#corpus_path = '/content/drive/MyDrive/Style-Transfer/Data/Corpus.npy'\n",
        "#np.save(corpus_path,np.array(corpus), allow_pickle=True)"
      ],
      "metadata": {
        "id": "JG3CXw1QCLVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#len(vocabulary_map)"
      ],
      "metadata": {
        "id": "rsVu2MitxaKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lengths"
      ],
      "metadata": {
        "id": "9YUQmWY5q0kp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f320ee19-6e64-459f-8c7e-667fe1190e00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[91130, 22265]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels1 = ['Kings and Generals' if i < 91616 else '3Blue1Brown' for i in range(sum(lengths))]\n",
        "\n"
      ],
      "metadata": {
        "id": "dQv_ymXMx9bX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = []\n",
        "for i in range(len(lengths)):\n",
        "  for j in range(lengths[i]):\n",
        "    labels.append(channels[i])\n",
        "\n",
        "labels==labels1"
      ],
      "metadata": {
        "id": "zY0v7kSVUkWF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7f765c6-e6b3-450f-fd36-cf129eb1d5d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_path = '/content/drive/MyDrive/Style-Transfer/Data/labels.npy'\n",
        "np.save(labels_path, labels)"
      ],
      "metadata": {
        "id": "R2fmJtTMzbo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "YhNMGmovhaK6",
        "outputId": "547d6b7f-edf4-44e6-ed62-c2695959d51d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Get token from index\n",
        "vocabulary_map.lookup_token(5)\n",
        "#Get index from token\n",
        "vocabulary_map['and']\n",
        "\n",
        "vocabulary_map['in']  #Word2index\n",
        "vocabulary_map.lookup_token(10)  #index2Word\n",
        "vocabulary_map.get_itos() #BOW"
      ],
      "metadata": {
        "id": "EZfwzhVZ5Z5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test = torch.load(vocab_path)      #Load Vocab Index\n",
        "test"
      ],
      "metadata": {
        "id": "PjbMVyNIWplT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff3b474f-1f35-4eaa-a4e5-2f77622a7760"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Vocab()"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evening out the number of the tokens in corpus, based on the shortest\n",
        "torch.manual_seed(42)\n",
        "\n",
        "temp = np.array(corpus)\n",
        "random_indices = np.random.choice(lengths[0]-1, size = lengths[0] - lengths[1], replace=False)\n",
        "temp_cut = np.delete(temp, obj=random_indices, axis=0)\n",
        "\n",
        "ordered_dict, vocabulary_map = make_vocabulary_map(temp_cut)\n",
        "\n",
        "\n",
        "labels = ['Kings and Generals' if i < lengths[1] else '3Blue1Brown' for i in range(sum(lengths))]\n",
        "\n",
        "corpus_path = '/content/drive/MyDrive/Style-Transfer/Data/Corpus.npy'\n",
        "np.save(corpus_path,temp_cut, allow_pickle=True)\n",
        "\n",
        "vocab_path = '/content/drive/MyDrive/Style-Transfer/Data/Vocabulary_obj.pth'#tensor object from torch\n",
        "torch.save(vocabulary_map, vocab_path)  #Save Vocab Index\n",
        "\n",
        "labels_path = '/content/drive/MyDrive/Style-Transfer/Data/labels.npy' # disparate labels from code\n",
        "np.save(labels_path, labels)"
      ],
      "metadata": {
        "id": "3YLZnWd_-wzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ordered_dict, vocabulary_map = make_vocabulary_map(temp_cut)\n",
        "vocab_path = '/content/drive/MyDrive/Style-Transfer/Data/Vocabulary_obj.pth'\n",
        "torch.save(vocabulary_map, vocab_path)  #Save Vocab Index"
      ],
      "metadata": {
        "id": "C3lHzwoJCfod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rAUmqSptARLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#44264//128"
      ],
      "metadata": {
        "id": "FimGkJf-EHF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# corpus_path = '/content/drive/MyDrive/Style-Transfer/Data/Corpus.npy'\n",
        "\n",
        "# test = np.load(corpus_path, allow_pickle = True)"
      ],
      "metadata": {
        "id": "RR3JV9Um_8PC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test[0]"
      ],
      "metadata": {
        "id": "xmuzyghz4I4Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f0a103a-fcaa-4d62-a563-03623e11cd16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['<sos>', 'at', 'pretty', 'much', 'every', ',', 'place', 'they',\n",
              "       'had', 'attempted', 'to', 'invade', '<pad>', '<pad>', '<pad>',\n",
              "       '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'], dtype='<U67')"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cut corpus into halves:\n",
        "\n"
      ],
      "metadata": {
        "id": "FrhwfFEk73mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7ILPOqJ0muO"
      },
      "outputs": [],
      "source": [
        "#list_period_delim = test_str.split(sep='.')\n",
        "#list_comma_delim = test_str.split(sep=',')\n",
        "#list_nltk_sentence_tokenizer = sent_tokenize(test_str)\n",
        "\n",
        "#print(len(list_period_delim))\n",
        "#print(len(list_comma_delim))\n",
        "#print(len(list_nltk_sentence_tokenizer))\n",
        "\n",
        "#[sent_tokenize(token) for token in list_comma_delim]\n",
        "\n",
        "#[len(token) for token in list_nltk_sentence_tokenizer]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
